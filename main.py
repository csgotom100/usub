import requests
import os
import re
import base64
import urllib.parse

def clean_text(text):
    if "<html" in text.lower():
        match = re.search(r'[A-Za-z0-9+/=]{50,}', text)
        return match.group(0) if match else ""
    return text

def main():
    if not os.path.exists('sources.txt'):
        print("âŒ æ²¡æ‰¾åˆ° sources.txt")
        return
    
    with open('sources.txt', 'r', encoding='utf-8') as f:
        urls = [l.strip() for l in f if l.startswith('http')]
    
    all_raw_content = []
    print(f"ðŸš€ å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªæº...")
    headers = {'User-Agent': 'clash-verge/1.0; Mozilla/5.0'}

    for idx, url in enumerate(urls):
        try:
            r = requests.get(url, headers=headers, timeout=10)
            if r.status_code == 200:
                content = clean_text(r.text.strip())
                if content:
                    all_raw_content.append(content)
                    print(f"   [{idx+1}] ä¸‹è½½æˆåŠŸ")
        except:
            continue

    if not all_raw_content:
        print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆå†…å®¹")
        return

    final_links = set()
    print(f"ðŸ“¦ æ­£åœ¨é€šè¿‡ API æå–èŠ‚ç‚¹ (å…± {len(all_raw_content)} æ®µ)...")
    
    for i, content in enumerate(all_raw_content):
        try:
            # æ ¸å¿ƒæ”¹è¿›ï¼šå°†å†…å®¹è½¬ä¸º Base64ï¼Œåˆ©ç”¨ SubConverter çš„ data åè®®
            # è·¯å¾„ä½¿ç”¨ /sub è€Œéž POST
            b64_data = base64.b64encode(content.encode('utf-8')).decode('utf-8')
            data_url = f"data:text/plain;base64,{b64_data}"
            
            # ä½¿ç”¨ GET è¯·æ±‚ï¼Œè¿™æ˜¯ SubConverter æœ€ç¨³å®šçš„è·¯å¾„
            api_url = f"http://127.0.0.1:25500/sub?target=v2ray&url={urllib.parse.quote(data_url)}&list=true"
            
            r = requests.get(api_url, timeout=30)
            
            if r.status_code == 200:
                lines = r.text.splitlines()
                added = 0
                for line in lines:
                    if line.strip() and "://" in line: 
                        final_links.add(line.strip())
                        added += 1
                print(f"   è¿›åº¦: {i+1}/{len(all_raw_content)} æˆåŠŸæå– {added} ä¸ª")
            else:
                print(f"   è·³è¿‡ç¬¬ {i+1} æ®µ: HTTP {r.status_code} (å°è¯•æ£€æŸ¥ API è·¯å¾„)")
        except Exception as e:
            print(f"   ç¬¬ {i+1} æ®µå¤„ç†å‡ºé”™: {e}")

    links_list = list(final_links)
    print(f"âœ… æ±‡æ€»åŽ»é‡å®Œæˆï¼Œå…± {len(links_list)} ä¸ªå”¯ä¸€èŠ‚ç‚¹")

    if not links_list:
        return

    # ä¿å­˜æ˜Žæ–‡åˆ—è¡¨
    with open("sub_v2ray.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(links_list))

    # ç”Ÿæˆæœ€ç»ˆ Clash
    print("ðŸŽ¨ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆ config.yaml...")
    try:
        final_b64 = base64.b64encode("\n".join(links_list).encode('utf-8')).decode('utf-8')
        final_data_url = f"data:text/plain;base64,{final_b64}"
        final_api = f"http://127.0.0.1:25500/sub?target=clash&url={urllib.parse.quote(final_data_url)}"
        
        r_clash = requests.get(final_api, timeout=60)
        if "proxies:" in r_clash.text:
            with open("config.yaml", "w", encoding="utf-8") as f:
                f.write(r_clash.text)
            print("ðŸŽ‰ å…¨éƒ¨å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æœ€ç»ˆè½¬æ¢å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
